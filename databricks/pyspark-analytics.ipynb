{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Window Function\n",
    "PySpark window functions are a set of SQL-like operations that allow you to perform calculations across a group of rows that are related to the current row, but without collapsing the rows into a single row. These functions are particularly useful for tasks such as ranking, aggregating over specific partitions, and calculating cumulative or rolling statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Features of PySpark Window Functions**\n",
    "\n",
    "1. Operate on a \"Window\" of Rows: Define a subset of data (the \"window\") for each row based on certain criteria like partitioning and ordering.\n",
    "1. Non-collapsing: Unlike groupBy, window functions keep the number of rows unchanged.\n",
    "1. SQL and Functional API: Can be used with both SQL queries and PySpark's DataFrame API.\n",
    "\n",
    "\n",
    "**Common Use Cases**<br>\n",
    "- **Ranking rows**: Assign ranks to rows within a partition.\n",
    "- **Cumulative calculations**: Compute running totals, averages, etc.\n",
    "- **Lag/Lead**: Access previous or next row values.\n",
    "- **Aggregations**: Perform operations like min, max, avg over a specified window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax**<br>\n",
    "Using a window function involves three steps:\n",
    "\n",
    "1. Define the Window:\n",
    "    - Partition: Specifies the grouping of rows.\n",
    "    - Order: Specifies the sorting within each partition.\n",
    "1. Apply the Function: Perform an operation (e.g., sum, rank).\n",
    "1. Use the Result: Add the calculated column to the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LP-CND1243S00:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Window Function</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x192e9107b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the SparkSession with a specific application name\n",
    "spark = (SparkSession.builder\n",
    "         .appName('PySpark Window Function')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking**: Rank products within each category based on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "]\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking functions\n",
    "\n",
    "PySparkâ€™s Window Ranking functions, such as row_number(), rank(), and dense_rank(), are used to assign unique identifiers or ranks to rows within a specific partition of a dataset. These functions operate over a window, which is a subset of data defined by a partitioning and ordering logic. They are useful for tasks like ordering, ranking, and identifying specific rows based on the specified conditions.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Partition**: Divides the dataset into groups based on one or more columns (e.g., department).\n",
    "- **Ordering**: Determines the sequence of rows within each partition (e.g., by salary).\n",
    "- **Sequential Assignment**: These functions assign numbers to rows in the order defined by the partition and sorting criteria.\n",
    "\n",
    "**Key Benefits**\n",
    "- **Enhanced Data Insights**: Easily analyze and compare rows within groups.\n",
    "- **Versatility**: Useful in real-world scenarios like leaderboard rankings, pagination, and top-N analysis.\n",
    "- **Control Over Ties**: Choose between rank and dense_rank depending on how you want to handle ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+----+----------+\n",
      "|employee_name|department|salary|row_number|rank|dense_rank|\n",
      "+-------------+----------+------+----------+----+----------+\n",
      "|        Maria|   Finance|  3000|         1|   1|         1|\n",
      "|        Scott|   Finance|  3300|         2|   2|         2|\n",
      "|          Jen|   Finance|  3900|         3|   3|         3|\n",
      "|        Kumar| Marketing|  2000|         1|   1|         1|\n",
      "|         Jeff| Marketing|  3000|         2|   2|         2|\n",
      "|        James|     Sales|  3000|         1|   1|         1|\n",
      "|       Robert|     Sales|  4100|         2|   2|         2|\n",
      "|         Saif|     Sales|  4100|         3|   2|         2|\n",
      "|      Michael|     Sales|  4600|         4|   4|         3|\n",
      "+-------------+----------+------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(window_spec_ex)) \\\n",
    "    .withColumn(\"rank\",rank().over(window_spec_ex)) \\\n",
    "    .withColumn(\"dense_rank\",dense_rank().over(window_spec_ex)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Selling Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: double (nullable = true)\n",
      "\n",
      "+----------+--------+------+----------+------+--------+\n",
      "|  Category| Product|Region|      Date| Sales|Quantity|\n",
      "+----------+--------+------+----------+------+--------+\n",
      "|Home Decor| Cushion|  East|2022-04-13| 388.1|     8.0|\n",
      "|  Clothing|  Jacket|  East|2023-12-02|278.27|     3.0|\n",
      "|  Clothing|   Jeans|  West|2023-01-08|280.61|     8.0|\n",
      "|    Sports|Football| South|2022-12-10|407.84|     6.0|\n",
      "|Home Decor| Cushion| South|2023-01-21|  50.0|     5.0|\n",
      "+----------+--------+------+----------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdf = spark.read.csv('dataset/sales_data_cleaned.csv', header=True, inferSchema=True).drop('Month')\n",
    "print(salesdf.count())\n",
    "salesdf.printSchema()\n",
    "salesdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+\n",
      "|   Category|        Product|TotalSales|\n",
      "+-----------+---------------+----------+\n",
      "|Electronics|     Headphones|   28898.0|\n",
      "|     Sports|       Yoga Mat|   47373.0|\n",
      "| Home Decor|           Lamp|   36884.0|\n",
      "|Electronics|Unknown Product|   45391.0|\n",
      "| Home Decor|        Cushion|   33569.0|\n",
      "|      Books|        Fiction|   46667.0|\n",
      "|      Books|         Comics|   56466.0|\n",
      "|     Sports|       Football|   52603.0|\n",
      "|Electronics|         Laptop|   46642.0|\n",
      "|      Books|    Non-Fiction|   53522.0|\n",
      "|Electronics|         Mobile|   49148.0|\n",
      "| Home Decor|        Curtain|   70236.0|\n",
      "|     Sports|  Tennis Racket|   86067.0|\n",
      "|   Clothing|         Jacket|   53495.0|\n",
      "|   Clothing|          Shirt|   63192.0|\n",
      "|   Clothing|          Jeans|   64855.0|\n",
      "+-----------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot_salesdf = (salesdf\n",
    "               .filter(\"Category != 'Unknown'\")\n",
    "               .groupBy('Category', 'Product')\n",
    "               .agg(\n",
    "                   round(sum(col(\"Sales\") * col(\"Quantity\"))).alias(\"TotalSales\")\n",
    "               ))\n",
    "tot_salesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|   Category|      Product|TotalSales|\n",
      "+-----------+-------------+----------+\n",
      "|     Sports|Tennis Racket|   86067.0|\n",
      "| Home Decor|      Curtain|   70236.0|\n",
      "|   Clothing|        Jeans|   64855.0|\n",
      "|      Books|       Comics|   56466.0|\n",
      "|Electronics|       Mobile|   49148.0|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\").desc())\n",
    "\n",
    "# Apply the ranking functions\n",
    "ranked_df = tot_salesdf.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "    \n",
    "(ranked_df\n",
    "    .filter(col(\"row_number\") == 1)\n",
    "    .select('Category', 'Product', 'TotalSales')\n",
    "    .sort(col('TotalSales').desc())\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ntile window function\n",
    "\n",
    "The ```ntile() ```window function in PySpark is used to distribute rows of data into a specified number of buckets or groups, based on the ordering of the rows within a partition. The rows are evenly divided into the given number of buckets, and each row is assigned a bucket number from 1 to n, where n is the number of buckets.\n",
    "\n",
    "**How ntile() works:**\n",
    "- The function takes a single argument, which is the number of buckets (n) to divide the data into.\n",
    "- It returns the bucket number for each row in the ordered set.\n",
    "- The rows are first ordered according to a specified column and then divided into n groups as evenly as possible.\n",
    "    - If the rows cannot be evenly divided, some buckets may contain one more row than others.\n",
    "\n",
    "**Use Cases for ntile():**\n",
    "- **Percentile Calculations**: Dividing data into quantiles like quartiles, deciles, etc., for analysis such as statistical summaries.\n",
    "- **Categorization**: Assigning categories to data points based on ranks, such as splitting data into high, medium, and low categories.\n",
    "- **Segmenting Data**: Segmenting users, customers, or employees based on certain metrics (e.g., income, sales performance, etc.) into equal-sized buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Each employee is assigned a Salary_Quartile based on their salary relative to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---------------+\n",
      "|employee_name|department|salary|Salary_Quartile|\n",
      "+-------------+----------+------+---------------+\n",
      "|      Michael|     Sales|  4600|              1|\n",
      "|       Robert|     Sales|  4100|              1|\n",
      "|         Saif|     Sales|  4100|              1|\n",
      "|          Jen|   Finance|  3900|              2|\n",
      "|        Scott|   Finance|  3300|              2|\n",
      "|        James|     Sales|  3000|              3|\n",
      "|        Maria|   Finance|  3000|              3|\n",
      "|         Jeff| Marketing|  3000|              4|\n",
      "|        Kumar| Marketing|  2000|              4|\n",
      "+-------------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Window specification (ordering by Salary in descending order)\n",
    "window_spec_ex = Window.orderBy(col(\"Salary\").desc())\n",
    "df.withColumn(\"Salary_Quartile\",ntile(4).over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+-----+\n",
      "|   Category|        Product|TotalSales|ntile|\n",
      "+-----------+---------------+----------+-----+\n",
      "|     Sports|  Tennis Racket|   86067.0|    1|\n",
      "| Home Decor|        Curtain|   70236.0|    1|\n",
      "|   Clothing|          Jeans|   64855.0|    1|\n",
      "|   Clothing|          Shirt|   63192.0|    1|\n",
      "|      Books|         Comics|   56466.0|    1|\n",
      "|      Books|    Non-Fiction|   53522.0|    1|\n",
      "|   Clothing|         Jacket|   53495.0|    1|\n",
      "|     Sports|       Football|   52603.0|    1|\n",
      "|Electronics|         Mobile|   49148.0|    2|\n",
      "|     Sports|       Yoga Mat|   47373.0|    2|\n",
      "|      Books|        Fiction|   46667.0|    2|\n",
      "|Electronics|         Laptop|   46642.0|    2|\n",
      "|Electronics|Unknown Product|   45391.0|    2|\n",
      "| Home Decor|           Lamp|   36884.0|    2|\n",
      "| Home Decor|        Cushion|   33569.0|    2|\n",
      "|Electronics|     Headphones|   28898.0|    2|\n",
      "+-----------+---------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\").desc())\n",
    "tot_salesdf.withColumn(\"ntile\",ntile(2).over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Distribution Window Function\n",
    "\n",
    "The ```cume_dist()``` window function in PySpark calculates the cumulative distribution of a value within a partition. It provides a measure of how the current row compares to all the other rows in the partition based on a specific ordering. It calculates the relative rank of each row in terms of its value in the specified partition, normalized between 0 and 1.\n",
    "\n",
    "i.e. cume_dist() gives the fraction of rows in the partition that have a value less than or equal to the current row's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|          CumeDist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "|        James|     Sales|  3000|              0.25|\n",
      "|       Robert|     Sales|  4100|              0.75|\n",
      "|         Saif|     Sales|  4100|              0.75|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply cume_dist function to calculate the cumulative distribution of salaries\n",
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"CumeDist\", cume_dist().over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+------------------+\n",
      "|   Category|        Product|TotalSales|          CumeDist|\n",
      "+-----------+---------------+----------+------------------+\n",
      "|      Books|        Fiction|   46667.0|0.3333333333333333|\n",
      "|      Books|    Non-Fiction|   53522.0|0.6666666666666666|\n",
      "|      Books|         Comics|   56466.0|               1.0|\n",
      "|   Clothing|         Jacket|   53495.0|0.3333333333333333|\n",
      "|   Clothing|          Shirt|   63192.0|0.6666666666666666|\n",
      "|   Clothing|          Jeans|   64855.0|               1.0|\n",
      "|Electronics|     Headphones|   28898.0|              0.25|\n",
      "|Electronics|Unknown Product|   45391.0|               0.5|\n",
      "|Electronics|         Laptop|   46642.0|              0.75|\n",
      "|Electronics|         Mobile|   49148.0|               1.0|\n",
      "| Home Decor|        Cushion|   33569.0|0.3333333333333333|\n",
      "| Home Decor|           Lamp|   36884.0|0.6666666666666666|\n",
      "| Home Decor|        Curtain|   70236.0|               1.0|\n",
      "|     Sports|       Yoga Mat|   47373.0|0.3333333333333333|\n",
      "|     Sports|       Football|   52603.0|0.6666666666666666|\n",
      "|     Sports|  Tennis Racket|   86067.0|               1.0|\n",
      "+-----------+---------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\"))\n",
    "tot_salesdf.withColumn(\"CumeDist\",cume_dist().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Window Function\n",
    "\n",
    "The ```lag()``` window function in PySpark is used to access data from a previous row in the same result set, without needing to perform a self-join. This function provides a way to compare the current row's value to previous row values based on a specified order within a partition.\n",
    "\n",
    "**Syntax:** ```df.withColumn(\"lag_column\", F.lag(column_name, offset, default_value).over(window_spec))```\n",
    "\n",
    "**Where:**\n",
    "- **column_name**: The column from which you want to retrieve the lagged value.\n",
    "- **offset**: (Optional) The number of rows before the current row to access. By default, it's 1.\n",
    "- **default_value**: (Optional) The value to return if there is no previous row. By default, it's None.\n",
    "- **window_spec**: Defines how to partition and order the data.\n",
    "\n",
    "**How it Works:**\n",
    "- The lag() function returns the value of the column from a previous row based on the specified offset (number of rows before the current row).\n",
    "- It requires a window specification that defines how the data should be partitioned and ordered.\n",
    "- If there is no previous row (e.g., for the first row), the lag() function returns the default value or null if no default value is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+----+\n",
      "|employee_name|department|salary| lag|lead|\n",
      "+-------------+----------+------+----+----+\n",
      "|        Maria|   Finance|  3000|NULL|3300|\n",
      "|        Scott|   Finance|  3300|3000|3900|\n",
      "|          Jen|   Finance|  3900|3300|NULL|\n",
      "|        Kumar| Marketing|  2000|NULL|3000|\n",
      "|         Jeff| Marketing|  3000|2000|NULL|\n",
      "|        James|     Sales|  3000|NULL|4100|\n",
      "|       Robert|     Sales|  4100|3000|4100|\n",
      "|         Saif|     Sales|  4100|4100|4600|\n",
      "|      Michael|     Sales|  4600|4100|NULL|\n",
      "+-------------+----------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lag\",lag(\"salary\").over(window_spec_ex)) \\\n",
    "    .withColumn(\"lead\",lead(\"salary\").over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the year 2023\n",
    "df_2023 = salesdf.filter(year(col(\"Date\")) == 2023)\n",
    "\n",
    "# Group by Month and calculate total sales\n",
    "df_monthly_sales = df_2023.groupBy(month(col(\"Date\")).alias(\"Month\")).agg(\n",
    "    round(sum(\"Sales\"),2).alias(\"TotalSales\")\n",
    ").sort('Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------+--------------+\n",
      "|Month|TotalSales|PreviousMonthSales|NextMonthSales|\n",
      "+-----+----------+------------------+--------------+\n",
      "|    1|   9980.59|              NULL|       8739.22|\n",
      "|    2|   8739.22|           9980.59|       9191.11|\n",
      "|    3|   9191.11|           8739.22|       7492.81|\n",
      "|    4|   7492.81|           9191.11|       9045.63|\n",
      "|    5|   9045.63|           7492.81|       7538.85|\n",
      "|    6|   7538.85|           9045.63|       9150.08|\n",
      "|    7|   9150.08|           7538.85|       7514.56|\n",
      "|    8|   7514.56|           9150.08|       8443.14|\n",
      "|    9|   8443.14|           7514.56|       6863.55|\n",
      "|   10|   6863.55|           8443.14|      10019.73|\n",
      "|   11|  10019.73|           6863.55|       6847.05|\n",
      "|   12|   6847.05|          10019.73|          NULL|\n",
      "+-----+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Window specification (order by Month)\n",
    "window_spec = Window.orderBy(\"Month\")\n",
    "\n",
    "# Apply lag() and lead() to get the previous and next month's sales\n",
    "df_with_lag_lead = df_monthly_sales.withColumn(\n",
    "    \"PreviousMonthSales\", lag(\"TotalSales\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"NextMonthSales\", lead(\"TotalSales\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "df_with_lag_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

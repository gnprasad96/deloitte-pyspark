{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Query Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c26cf7-a93e-4d36-893a-88a43c9cc431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data into Data frame. Reading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c3cec6-05c1-4d81-893e-7b6bb73b6749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transactions_file = \"dbfs:/mnt/data/data/data_skew/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4e1dfe-57d8-40cf-8ed9-8fbe3d0dae8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city       |\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|10   |7  |Entertainment|10.42 |boston     |\n|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|3    |27 |Motor/Travel |44.34 |portland   |\n|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|4    |11 |Entertainment|3.18  |chicago    |\n|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|2    |22 |Groceries    |268.97|los_angeles|\n|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|10   |16 |Entertainment|2.66  |chicago    |\n+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c30966-fbbf-4064-a881-cafaa61af2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_file = \"dbfs:/mnt/data/data/data_skew/customers.parquet\"\n",
    "df_customers = spark.read.parquet(customers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b988763-5a53-430d-af2c-b5bf73a9933e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+-----------+\n|cust_id   |name         |age|gender|birthday  |zip  |city       |\n+----------+-------------+---+------+----------+-----+-----------+\n|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston     |\n|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago    |\n|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver     |\n|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles|\n|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego  |\n+----------+-------------+---+------+----------+-----+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7695a8bd-e075-4aff-9df6-e8c61bfd737e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84132af1-661f-4fec-a285-6289412d87c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_narrow_transform = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"city\") == \"boston\")\n",
    "    .withColumn(\"first_name\", F.split(\"name\", \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", F.split(\"name\", \" \").getItem(1))\n",
    "    .withColumn(\"age\", F.col(\"age\") + F.lit(5))\n",
    "    .select(\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cf1bcf-5569-49be-9e7e-9b5c6f705182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+------+---------+\n|cust_id   |first_name|last_name|age |gender|birthday |\n+----------+----------+---------+----+------+---------+\n|C007YEYTX9|Aaron     |Abbott   |39.0|Female|7/13/1991|\n|C08XAQUY73|Aaron     |Lambert  |59.0|Female|11/5/1966|\n|C094P1VXF9|Aaron     |Lindsey  |29.0|Male  |9/21/1990|\n|C097SHE1EF|Aaron     |Lopez    |27.0|Female|4/18/2001|\n|C0DTC6436T|Aaron     |Schwartz |57.0|Female|7/9/1962 |\n+----------+----------+---------+----+------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Trigger and action\n",
    "df_narrow_transform.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c32a218-b96d-41b3-a495-e9808a2c1629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project ['cust_id, 'first_name, 'last_name, 'age, 'gender, 'birthday]\n+- Project [cust_id#70, name#71, (cast(age#72 as double) + cast(5 as double)) AS age#133, gender#73, birthday#74, zip#75, city#76, first_name#114, last_name#123]\n   +- Project [cust_id#70, name#71, age#72, gender#73, birthday#74, zip#75, city#76, first_name#114, split(name#71,  , -1)[1] AS last_name#123]\n      +- Project [cust_id#70, name#71, age#72, gender#73, birthday#74, zip#75, city#76, split(name#71,  , -1)[0] AS first_name#114]\n         +- Filter (city#76 = boston)\n            +- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, first_name: string, last_name: string, age: double, gender: string, birthday: string\nProject [cust_id#70, first_name#114, last_name#123, age#133, gender#73, birthday#74]\n+- Project [cust_id#70, name#71, (cast(age#72 as double) + cast(5 as double)) AS age#133, gender#73, birthday#74, zip#75, city#76, first_name#114, last_name#123]\n   +- Project [cust_id#70, name#71, age#72, gender#73, birthday#74, zip#75, city#76, first_name#114, split(name#71,  , -1)[1] AS last_name#123]\n      +- Project [cust_id#70, name#71, age#72, gender#73, birthday#74, zip#75, city#76, split(name#71,  , -1)[0] AS first_name#114]\n         +- Filter (city#76 = boston)\n            +- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Optimized Logical Plan ==\nProject [cust_id#70, split(name#71,  , 2)[0] AS first_name#114, split(name#71,  , 3)[1] AS last_name#123, (cast(age#72 as double) + 5.0) AS age#133, gender#73, birthday#74]\n+- Filter (isnotnull(city#76) AND (city#76 = boston))\n   +- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Physical Plan ==\n*(1) Project [cust_id#70, split(name#71,  , 2)[0] AS first_name#114, split(name#71,  , 3)[1] AS last_name#123, (cast(age#72 as double) + 5.0) AS age#133, gender#73, birthday#74]\n+- *(1) Filter (isnotnull(city#76) AND (city#76 = boston))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet [cust_id#70,name#71,age#72,gender#73,birthday#74,city#76] Batched: true, DataFilters: [isnotnull(city#76), (city#76 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,city:string>\n\n"
     ]
    }
   ],
   "source": [
    "# See the query plan\n",
    "df_narrow_transform.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683b2005-d9f3-4546-a093-d5345eddee32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wide Transformations\n",
    "1. Repartition\n",
    "2. Coalesce\n",
    "3. Joins\n",
    "4. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5986faa1-00c4-47f1-99c0-2295784c65be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81510614-4c27-454c-9c62-5edc8a0351de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 12"
     ]
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a859fd-a63d-4d49-92c4-f31bb2332172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nRepartition 24, true\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartition 24, true\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Optimized Logical Plan ==\nRepartition 24, true\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange RoundRobinPartitioning(24), REPARTITION_BY_NUM, [plan_id=75]\n   +- FileScan parquet [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n\n"
     ]
    }
   ],
   "source": [
    "df_transactions.repartition(24).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0758f8-56cb-4920-a068-fd89ea6d13eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d670146-551d-4bd5-9e88-631e6f3f4932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nRepartition 5, false\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\nRepartition 5, false\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Optimized Logical Plan ==\nRepartition 5, false\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Physical Plan ==\nCoalesce 5\n+- *(1) ColumnarToRow\n   +- FileScan parquet [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n\n"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(5).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8897a230-b190-4982-88fc-0f52bd1dcf10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why doesn't `.coalesce()` explicitly show the partitioning scheme?\n",
    "\n",
    "`.coalesce` doesn't show the partitioning scheme e.g. `RoundRobinPartitioning` because: \n",
    "- The operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling.\n",
    "- Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by `.coalesce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7763e5a7-765a-4edd-ae58-fa826157ccb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278289a5-047f-4aa3-b069-8a36bac558ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46af46e-e1d8-4d1d-96a7-d8290292805c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e76cf0-1f4e-4a50-9c2b-4e2cc8ab2951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Join UsingJoin(Inner,Buffer(cust_id))\n:- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n+- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\nProject [cust_id#2, start_date#3, end_date#4, txn_id#5, date#6, year#7, month#8, day#9, expense_type#10, amt#11, city#12, name#71, age#72, gender#73, birthday#74, zip#75, city#76]\n+- Join Inner, (cust_id#2 = cust_id#70)\n   :- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n   +- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Optimized Logical Plan ==\nProject [cust_id#2, start_date#3, end_date#4, txn_id#5, date#6, year#7, month#8, day#9, expense_type#10, amt#11, city#12, name#71, age#72, gender#73, birthday#74, zip#75, city#76]\n+- Join Inner, (cust_id#2 = cust_id#70)\n   :- Filter isnotnull(cust_id#2)\n   :  +- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n   +- Filter isnotnull(cust_id#70)\n      +- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [cust_id#2, start_date#3, end_date#4, txn_id#5, date#6, year#7, month#8, day#9, expense_type#10, amt#11, city#12, name#71, age#72, gender#73, birthday#74, zip#75, city#76]\n   +- SortMergeJoin [cust_id#2], [cust_id#70], Inner\n      :- Sort [cust_id#2 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(cust_id#2, 200), ENSURE_REQUIREMENTS, [plan_id=124]\n      :     +- Filter isnotnull(cust_id#2)\n      :        +- FileScan parquet [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] Batched: true, DataFilters: [isnotnull(cust_id#2)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n      +- Sort [cust_id#70 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(cust_id#70, 200), ENSURE_REQUIREMENTS, [plan_id=125]\n            +- Filter isnotnull(cust_id#70)\n               +- FileScan parquet [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] Batched: true, DataFilters: [isnotnull(cust_id#70)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe1229a-7c93-478e-a421-980fa908682e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38bc715-bf17-42e7-8c5a-cad9def21dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- cust_id: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- end_date: string (nullable = true)\n |-- txn_id: string (nullable = true)\n |-- date: string (nullable = true)\n |-- year: string (nullable = true)\n |-- month: string (nullable = true)\n |-- day: string (nullable = true)\n |-- expense_type: string (nullable = true)\n |-- amt: string (nullable = true)\n |-- city: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "558187d7-fde6-4a90-a2da-2a1356f4ada8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3095738-ce37-4501-b27a-af11857baf12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_city_counts = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161c1c61-be35-4b98-9569-850d789dcb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['city], ['city, count(1) AS count#209L]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Analyzed Logical Plan ==\ncity: string, count: bigint\nAggregate [city#12], [city#12, count(1) AS count#209L]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Optimized Logical Plan ==\nAggregate [city#12], [city#12, count(1) AS count#209L]\n+- Project [city#12]\n   +- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[city#12], functions=[finalmerge_count(merge count#213L) AS count(1)#208L], output=[city#12, count#209L])\n   +- Exchange hashpartitioning(city#12, 200), ENSURE_REQUIREMENTS, [plan_id=200]\n      +- HashAggregate(keys=[city#12], functions=[partial_count(1) AS count#213L], output=[city#12, count#213L])\n         +- FileScan parquet [city#12] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<city:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_city_counts.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ab7b72-4c64-4b0f-95f6-c968cc720605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_txn_amt_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"city\")\n",
    "    .agg(F.sum(\"amt\").alias(\"txn_amt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94935d80-bb57-4c97-965b-6a14027d94a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['city], ['city, sum('amt) AS txn_amt#229]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Analyzed Logical Plan ==\ncity: string, txn_amt: double\nAggregate [city#12], [city#12, sum(cast(amt#11 as double)) AS txn_amt#229]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Optimized Logical Plan ==\nAggregate [city#12], [city#12, sum(cast(amt#11 as double)) AS txn_amt#229]\n+- Project [amt#11, city#12]\n   +- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[city#12], functions=[finalmerge_sum(merge sum#233) AS sum(cast(amt#11 as double))#228], output=[city#12, txn_amt#229])\n   +- Exchange hashpartitioning(city#12, 200), ENSURE_REQUIREMENTS, [plan_id=246]\n      +- HashAggregate(keys=[city#12], functions=[partial_sum(cast(amt#11 as double)) AS sum#233], output=[city#12, sum#233])\n         +- FileScan parquet [amt#11,city#12] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<amt:string,city:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_txn_amt_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329b30d7-bb82-4fe0-ab0a-4d81e847d24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GroupBy Count Distinct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b379d297-b9fe-4bbd-8c87-5c8aecc47a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_txn_per_city = (\n",
    "    df_transactions\n",
    "    .groupBy(\"cust_id\")\n",
    "    .agg(F.countDistinct(\"city\").alias(\"city_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e93fd9-73ad-4a49-bb0c-9a90f4c4310e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|cust_id   |city_count|\n+----------+----------+\n|CPP8BY8U93|10        |\n|CYB8BX9LU1|10        |\n|CFRT841CCD|10        |\n|CA0TSNMYDK|10        |\n|COZ8NONEVZ|10        |\n+----------+----------+\nonly showing top 5 rows\n\n== Parsed Logical Plan ==\n'Aggregate ['cust_id], ['cust_id, 'count(distinct 'city) AS city_count#252]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, city_count: bigint\nAggregate [cust_id#2], [cust_id#2, count(distinct city#12) AS city_count#252L]\n+- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Optimized Logical Plan ==\nAggregate [cust_id#2], [cust_id#2, count(distinct city#12) AS city_count#252L]\n+- Project [cust_id#2, city#12]\n   +- Relation [cust_id#2,start_date#3,end_date#4,txn_id#5,date#6,year#7,month#8,day#9,expense_type#10,amt#11,city#12] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[cust_id#2], functions=[finalmerge_count(distinct merge count#263L) AS count(city#12)#253L], output=[cust_id#2, city_count#252L])\n   +- Exchange hashpartitioning(cust_id#2, 200), ENSURE_REQUIREMENTS, [plan_id=452]\n      +- HashAggregate(keys=[cust_id#2], functions=[partial_count(distinct city#12) AS count#263L], output=[cust_id#2, count#263L])\n         +- HashAggregate(keys=[cust_id#2, city#12], functions=[], output=[cust_id#2, city#12])\n            +- Exchange hashpartitioning(cust_id#2, city#12, 200), ENSURE_REQUIREMENTS, [plan_id=448]\n               +- HashAggregate(keys=[cust_id#2, city#12], functions=[], output=[cust_id#2, city#12])\n                  +- FileScan parquet [cust_id#2,city#12] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/transactions.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,city:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_txn_per_city.show(5, False)\n",
    "df_txn_per_city.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f947ecbe-b916-4a7a-9dee-8c637a918e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Interesting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c153be-6615-40af-9bc7-435667c521a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why is a filter step present despite predicate pushdown? \n",
    "\n",
    "This is largely due to the way `Spark's Catalyst Optimizer` works. Specifically, due to two separate stages of the query optimization process: Physical Planning and Logical Planning.\n",
    "\n",
    "- **Logical Planning**: Catalyst optimizer simplifies the unresolved logical plan (which represents the user's query) by applying various rule-based optimizations. This includes `predicate pushdown`, `projection pushdown` where filter conditions and column projections are moved as close to the data source as possible.\n",
    "\n",
    "- **Physical Planning** phase is where the logical plan is translated into one or more physical plans, which can actually be executed on the cluster. This includes operations like file `scans`, `filters`, `projections`, etc.\n",
    "\n",
    "In this case, during the logical planning phase, the predicate (`F.col(\"city\") == \"boston\"`) has been pushed down and will be applied during the scan of the Parquet file (`PushedFilters: [IsNotNull(city), EqualTo(city,boston)]`), thus improving performance.\n",
    "\n",
    "Now, during the physical planning phase, the same filter condition (`+- *(1) Filter (isnotnull(city#73) AND (city#73 = boston))`) is applied again to the data that's been loaded into memory. This is because of the following reasons:\n",
    "\n",
    "1. **Guaranteed Correctness:** It might seem **redundant**, but remember that not all data sources can handle pushed-down predicates, and not all predicates can be pushed down. Therefore, **even if a predicate is pushed down to the data source, Spark still includes the predicate in the physical plan** to cover cases where the data source might not have been able to fully apply the predicate. This is Spark's way of making sure the correct data is always returned, no matter the capabilities of the data source.\n",
    "\n",
    "2. **No Assumptions**: Spark's Catalyst optimizer doesn't make assumptions about the data source's ability to handle pushed-down predicates. The optimizer aims to generate plans that return correct results across a wide range of scenarios. Even if the filter is pushed down, Spark does not have the feedback from data source whether the pushdown was successful or not, so it includes the filter operation in the physical plan as well.\n",
    "\n",
    "It is more of a **fail-safe mechanism** to ensure data **integrity** and **correctness**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ee2e88-ceea-4469-8a9a-42e89caac3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### In what cases will predicate pushdown not work?\n",
    "\n",
    "2 Examples where **filter pushdown** will not work:\n",
    "\n",
    "1. **Complex Data Types**: Spark's Parquet data source does not push down filters that involve **complex types**, such as **arrays**, **maps**, and **structs**. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    "\n",
    "+----------+-----------------------------+\n",
    "|Name      |properties                   |\n",
    "+----------+-----------------------------+\n",
    "|Afaque    |[eye -> black, hair -> black]|\n",
    "|Naved     |[eye ->, hair -> brown]      |\n",
    "|Ali       |[eye -> black, hair -> red]  |\n",
    "|Amaan     |[eye -> grey, hair -> grey]  |\n",
    "|Omaira    |[eye -> , hair -> brown]     |\n",
    "+----------+-----------------------------+\n",
    "```\n",
    "\n",
    "```python\n",
    "df.filter(df.properties.getItem(\"eye\") == \"brown\").show()\n",
    "```\n",
    "\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) Filter (metadata#123[key] = value)\n",
    "+- *(1) ColumnarToRow\n",
    "   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n",
    "```\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "3. Unsupported Expressions: \n",
    "\n",
    "In Spark, `Parquet` data source does not support pushdown for filters involving a `.cast` operation. The reason for this behaviour is as follows:\n",
    "- `.cast` changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.\n",
    "\n",
    "**Note**: This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the `.cast` filter could potentially be pushed down to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65fc43c1-6d71-4dde-a787-a36db40ff3d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example of operation where filter pushdown doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730a771e-3d5e-45e9-811f-72b01590963f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+----------+-----+------------+\n|cust_id   |name          |age|gender|birthday  |zip  |city        |\n+----------+--------------+---+------+----------+-----+------------+\n|C01BKUFRHA|Aaron Becker  |54 |Male  |11/24/1979|40284|san_diego   |\n|C01WMZQ7PN|Aaron Brady   |51 |Female|8/20/1994 |52204|philadelphia|\n|C021567NJZ|Aaron Briggs  |57 |Male  |3/10/1990 |22008|philadelphia|\n|C02JNTM46B|Aaron Chambers|51 |Male  |1/6/2001  |63337|new_york    |\n|C030A69V1L|Aaron Clarke  |55 |Male  |4/28/1999 |77176|philadelphia|\n+----------+--------------+---+------+----------+-----+------------+\nonly showing top 5 rows\n\n== Parsed Logical Plan ==\n'Filter (cast('age as int) > 50)\n+- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Analyzed Logical Plan ==\ncust_id: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\nFilter (cast(age#72 as int) > 50)\n+- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(age#72) AND (cast(age#72 as int) > 50))\n+- Relation [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] parquet\n\n== Physical Plan ==\n*(1) Filter (isnotnull(age#72) AND (cast(age#72 as int) > 50))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [cust_id#70,name#71,age#72,gender#73,birthday#74,zip#75,city#76] Batched: true, DataFilters: [isnotnull(age#72), (cast(age#72 as int) > 50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/data/data/data_skew/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(age)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_customer_gt_50 = (\n",
    "    df_customers\n",
    "    .filter(F.col(\"age\").cast(\"int\") > 50)\n",
    ")\n",
    "df_customer_gt_50.show(5, False)\n",
    "df_customer_gt_50.explain(True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Reading-Query-plans",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
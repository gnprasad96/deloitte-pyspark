{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91173802-6a49-4a37-8123-9a3e0f221c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use aggregate functions\n",
    "\n",
    "**Data Source**\n",
    "\n",
    "* English Wikipedia pageviews by second\n",
    "* Size on Disk: ~255 MB\n",
    "* Type: Parquet files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Introduce the various aggregate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57d99dd-77e2-4200-a0d4-97cf825312fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Create DataFrame from Dummy Data\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LP-CND1243S00:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create DataFrame from Dummy Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20060a23a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e30287-4fe2-486b-93e0-27f04d52fc1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\n",
    "\n",
    "This data uses the **Pageviews By Seconds** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b87d86fc-c4db-4b46-9173-71dc7ae3fc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "partitions = 8\n",
    "\n",
    "# Make sure wide operations don't repartition to 200\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a504b66-b38e-43d6-91ad-db969657d2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The directory containing our parquet files.\n",
    "parquetFile = \"../dataset/pageviews_by_second.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ebebe0-48b9-4433-b548-65f3ed523e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our initial DataFrame. We can let it infer the \n",
    "# schema because the cost for parquet files is really low.\n",
    "initialDF = (spark.read\n",
    "  .option(\"inferSchema\", \"true\") # The default, but not costly w/Parquet\n",
    "  .parquet(parquetFile)          # Read the data in\n",
    "  .repartition(partitions)       # From 7 >>> 8 partitions\n",
    "  .cache()                       # Cache the expensive operation\n",
    ")\n",
    "# materialize the cache\n",
    "initialDF.count()\n",
    "\n",
    "# rename the timestamp column and cast to a timestamp data type\n",
    "pageviewsDF = (initialDF\n",
    "  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n",
    "  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n",
    ")\n",
    "\n",
    "# cache the transformations on our new DataFrame by marking the DataFrame as cached and then materialize the result\n",
    "pageviewsDF.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0053e1f4-7140-4b4a-bdc3-32aee474632c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+\n",
      "|         capturedAt|   site|requests|\n",
      "+-------------------+-------+--------+\n",
      "|2015-03-22 22:41:36| mobile|    1667|\n",
      "|2015-03-16 08:51:07|desktop|    2223|\n",
      "|2015-03-17 07:13:16|desktop|    2189|\n",
      "|2015-03-16 05:54:59| mobile|    1097|\n",
      "|2015-03-17 14:21:38| mobile|    1342|\n",
      "|2015-03-23 17:43:40|desktop|    3202|\n",
      "|2015-03-18 00:44:12| mobile|    1524|\n",
      "|2015-03-21 05:10:01| mobile|    1156|\n",
      "|2015-03-19 12:11:03| mobile|    1096|\n",
      "|2015-03-23 11:43:22| mobile|     994|\n",
      "+-------------------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pageviewsDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a535bd-ee07-427c-9bcb-6e35ce501d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) groupBy()\n",
    "\n",
    "Aggregating data is one of the more common tasks when working with big data.\n",
    "* How many customers are over 65?\n",
    "* What is the ratio of men to women?\n",
    "* Group all emails by their sender.\n",
    "\n",
    "The function `groupBy()` is one tool that we can use for this purpose.\n",
    "\n",
    "If you look at the API docs, `groupBy(..)` is described like this:\n",
    "> Groups the Dataset using the specified columns, so that we can run aggregation on them.\n",
    "\n",
    "This function is a **wide** transformation - it will produce a shuffle and conclude a stage boundary.\n",
    "\n",
    "Unlike all of the other transformations we've seen so far, this transformation does not return a `DataFrame`.\n",
    "* In Scala it returns `RelationalGroupedDataset`\n",
    "* In Python it returns `GroupedData`\n",
    "\n",
    "This is because the call `groupBy(..)` is only 1/2 of the transformation.\n",
    "\n",
    "To see the other half, we need to take a look at it's return type, `RelationalGroupedDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7cb51c0-acb4-4c06-ac2c-2886293dd3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RelationalGroupedDataset\n",
    "\n",
    "If we take a look at the API docs for `RelationalGroupedDataset`, we can see that it supports the following aggregations:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `avg(..)` | Compute the mean value for each numeric columns for each group. |\n",
    "| `count(..)` | Count the number of rows for each group. |\n",
    "| `sum(..)` | Compute the sum for each numeric columns for each group. |\n",
    "| `min(..)` | Compute the min value for each numeric column for each group. |\n",
    "| `max(..)` | Compute the max value for each numeric columns for each group. |\n",
    "| `mean(..)` | Compute the average value for each numeric columns for each group. |\n",
    "| `agg(..)` | Compute aggregates by specifying a series of aggregate columns. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7677141-5414-431d-9858-2778cf7bfe9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Together, `groupBy(..)` and `RelationalGroupedDataset` (or `GroupedData` in Python) give us what we need to answer some basic questions.\n",
    "\n",
    "For Example, how many more requests did the desktop site receive than the mobile site receive?\n",
    "\n",
    "For this all we need to do is group all records by **site** and then sum all the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4fe614-32ee-4e25-9659-0c9728ae5d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   site|sum(requests)|\n",
      "+-------+-------------+\n",
      "| mobile|   4605797962|\n",
      "|desktop|   8737180972|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .groupBy( col(\"site\") )\n",
    "  .sum()\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [site], value: [capturedAt: timestamp, site: string ... 1 more field], type: GroupBy]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageviewsDF.groupBy(col(\"site\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6732fd3-0623-452e-ab76-ef042bd814e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notice above that we didn't actually specify which column we were summing....\n",
    "\n",
    "In this case you will actually receive a total for all numerical values.\n",
    "\n",
    "There is a performance catch to that - if I have 2, 5, 10? columns, then they will all be summed and I may only need one.\n",
    "\n",
    "I can first reduce my columns to those that I wanted or I can simply specify which column(s) to sum up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dafa2b0b-bc3e-4857-9199-1c76b2fa295a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   site|sum(requests)|\n",
      "+-------+-------------+\n",
      "| mobile|   4605797962|\n",
      "|desktop|   8737180972|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .groupBy( col(\"site\") )\n",
    "  .sum(\"requests\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "284c0fef-48e4-423d-98b9-8107859c2610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And because I don't like the resulting column name, **sum(requests)** I can easily rename it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04c3b2c-f719-4724-b3f4-8a224cde8ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   site|totalRequests|\n",
      "+-------+-------------+\n",
      "| mobile|   4605797962|\n",
      "|desktop|   8737180972|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .groupBy( col(\"site\") )\n",
    "  .sum(\"requests\")\n",
    "  .withColumnRenamed(\"sum(requests)\", \"totalRequests\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15741292-4215-419f-b356-952f103063bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How about the total number of requests per site? mobile vs desktop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f72e07-d73b-4ce8-b199-a3ff4d107f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   site|  count|\n",
      "+-------+-------+\n",
      "| mobile|3600000|\n",
      "|desktop|3600000|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .groupBy( col(\"site\") )\n",
    "  .count()\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378598ef-ca3d-43a5-80f1-4a09d3597332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) sum(), count(), avg(), min(), max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68027fa5-9ba6-426c-b2df-ca822bd35ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `groupBy(..)` operation is not our only option for aggregating.\n",
    "\n",
    "The `...sql.functions` package actually defines a large number of aggregate functions\n",
    "* `org.apache.spark.sql.functions` in the case of Scala & Java\n",
    "* `pyspark.sql.functions` in the case of Python\n",
    "\n",
    "\n",
    "Let's take a look at this in the Scala API docs (only because the documentation is a little easier to read)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13dac2e7-3703-4217-8f92-ca1c546d99f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a look at our last two examples... \n",
    "\n",
    "We saw the count of records and the sum of records.\n",
    "\n",
    "Let's take do this a slightly different way...\n",
    "\n",
    "This time with the `...sql.functions` operations.\n",
    "\n",
    "And just for fun, let's throw in the average, minimum and maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91fef078-4899-44e0-92d3-d3f7acb9c005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "|sum(requests)|count(requests)|     avg(requests)|min(requests)|max(requests)|\n",
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "|   4605797962|        3600000|1279.3883227777778|          645|         3292|\n",
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "\n",
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "|sum(requests)|count(requests)|     avg(requests)|min(requests)|max(requests)|\n",
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "|   8737180972|        3600000|2426.9947144444445|         1312|         5695|\n",
      "+-------------+---------------+------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .filter(\"site = 'mobile'\")\n",
    "  .select( sum( col(\"requests\")), count(col(\"requests\")), avg(col(\"requests\")), min(col(\"requests\")), max(col(\"requests\")) )\n",
    "  .show()\n",
    ")\n",
    "          \n",
    "(pageviewsDF\n",
    "  .filter(\"site = 'desktop'\")\n",
    "  .select( sum( col(\"requests\")), count(col(\"requests\")), avg(col(\"requests\")), min(col(\"requests\")), max(col(\"requests\")) )\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca44e6d7-2884-49a4-8c43-b5fe86977d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And let's just address one more pet-peeve...\n",
    "\n",
    "Was that 3.6M records or 360K records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51146d08-b9ca-4829-a924-a8913e229ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------+---+-----+\n",
      "|          sum|    count|     avg|min|  max|\n",
      "+-------------+---------+--------+---+-----+\n",
      "|4,605,797,962|3,600,000|1,279.39|645|3,292|\n",
      "+-------------+---------+--------+---+-----+\n",
      "\n",
      "+-------------------------------+---------------------------------+-------------------------------+-------------------------------+-------------------------------+\n",
      "|format_number(sum(requests), 0)|format_number(count(requests), 0)|format_number(avg(requests), 2)|format_number(min(requests), 0)|format_number(max(requests), 0)|\n",
      "+-------------------------------+---------------------------------+-------------------------------+-------------------------------+-------------------------------+\n",
      "|                  8,737,180,972|                        3,600,000|                       2,426.99|                          1,312|                          5,695|\n",
      "+-------------------------------+---------------------------------+-------------------------------+-------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF\n",
    "  .filter(\"site = 'mobile'\")\n",
    "  .select( \n",
    "    format_number(sum(col(\"requests\")), 0).alias(\"sum\"), \n",
    "    format_number(count(col(\"requests\")), 0).alias(\"count\"), \n",
    "    format_number(avg(col(\"requests\")), 2).alias(\"avg\"), \n",
    "    format_number(min(col(\"requests\")), 0).alias(\"min\"), \n",
    "    format_number(max(col(\"requests\")), 0).alias(\"max\") \n",
    "  )\n",
    "  .show()\n",
    ")\n",
    "\n",
    "(pageviewsDF\n",
    "  .filter(\"site = 'desktop'\")\n",
    "  .select( \n",
    "    format_number(sum(col(\"requests\")), 0), \n",
    "    format_number(count(col(\"requests\")), 0), \n",
    "    format_number(avg(col(\"requests\")), 2), \n",
    "    format_number(min(col(\"requests\")), 0), \n",
    "    format_number(max(col(\"requests\")), 0) \n",
    "  )\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+--------+-----+-----+\n",
      "|   site|          sum|    count|     avg|  min|  max|\n",
      "+-------+-------------+---------+--------+-----+-----+\n",
      "| mobile|4,605,797,962|3,600,000|1,279.39|  645|3,292|\n",
      "|desktop|8,737,180,972|3,600,000|2,426.99|1,312|5,695|\n",
      "+-------+-------------+---------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(pageviewsDF.groupBy(\"site\")\n",
    " .agg(\n",
    "    format_number(sum(col(\"requests\")), 0).alias(\"sum\"),\n",
    "    format_number(count(col(\"requests\")), 0).alias(\"count\"), \n",
    "    format_number(avg(col(\"requests\")), 2).alias(\"avg\"), \n",
    "    format_number(min(col(\"requests\")), 0).alias(\"min\"), \n",
    "    format_number(max(col(\"requests\")), 0).alias(\"max\") \n",
    " ).show()\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.Use-Aggregate-Functions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "740098b2-5a96-4ab8-9419-03b696af14de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# De-Duping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ef5e8b-9407-4da7-918d-0e1bca300d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "\n",
    "In this exercise, we're doing ETL on a file we've received from some customer. That file contains data about people, including:\n",
    "\n",
    "* first, middle and last names\n",
    "* gender\n",
    "* birth date\n",
    "* Social Security number\n",
    "* salary\n",
    "\n",
    "But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n",
    "\n",
    "* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\"). \n",
    "* The Social Security numbers aren't consistent, either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n",
    "\n",
    "The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. (The salaries will match, as well,\n",
    "and the Social Security Numbers *would* match, if they were somehow put in the same format).\n",
    "\n",
    "Your job is to remove the duplicate records. The specific requirements of your job are:\n",
    "\n",
    "* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n",
    "* Preserve the data format of the columns. For example, if you write the first name column in all lower-case, you haven't met this requirement.\n",
    "* Write the result as a Parquet file, as designated by *destFile*.\n",
    "* The final Parquet \"file\" must contain multiple part files (ending in \".parquet\").\n",
    "\n",
    "**Hint:** <br/>\n",
    "The initial dataset contains 103,000 records.<br/>\n",
    "The de-duplicated result haves 100,000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "483fe3ba-6338-4b15-a476-9c246904762f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Create DataFrame from Dummy Data\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LP-CND1243S00:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create DataFrame from Dummy Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2749ec2fc10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e241bfe7-1a8c-454d-89ea-2124c65527a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sourceFile = \"../dataset/people-with-dups.txt\"\n",
    "destFile = \"../dataset/out/people.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the ```shuffle.partitions```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98fc28c0-7a2b-40c2-9baa-0625fd747c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropDuplicates() will likely introduce a shuffle, so it helps to reduce the number of post-shuffle partitions.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853e6e05-b5f1-42a1-bd2f-ec2bec720f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Okay, now we can read this thing.\n",
    "\n",
    "df = (spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \":\")\n",
    "    .csv(sourceFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+------+----------+------+-----------+\n",
      "|firstName|middleName|    lastName|gender| birthDate|salary|        ssn|\n",
      "+---------+----------+------------+------+----------+------+-----------+\n",
      "|    Angla|     Melba|   Hartzheim|     F|1938-07-26| 13199|935-27-4276|\n",
      "|   Rachel|    Marlin|   Borremans|     F|1923-02-23| 67070|996-41-8616|\n",
      "| Madaline|  Shawanda|    Piszczek|     F|1996-03-17|183944|963-87-9974|\n",
      "|      Siu|   Cherrie|     Lechelt|     F|2012-07-24|148331|906-85-3202|\n",
      "|   Cheree|  Dorethea|    Anspaugh|     F|1985-01-17|278860|961-36-6578|\n",
      "|      See|    Sharen|     Howryla|     F|1979-12-30|169570|925-12-1644|\n",
      "|   Kattie|    Sammie|       Ercek|     F|2002-07-26|211993|996-32-1564|\n",
      "|  Bernard|    Reggie|      Coache|     M|1960-06-23| 53020|941-56-6401|\n",
      "|   Cordie|      Cara|     Sheilds|     F|2007-02-08|219449|950-98-5411|\n",
      "|   Audrey|   Lorrine|    Sprewell|     F|1932-10-25|283164|997-53-7925|\n",
      "|    ARLEN|    HAYDEN|     CARVILL|     M|1986-05-24| 66754|  918339442|\n",
      "|    Fritz|     Louis|Wechselblatt|     M|2000-01-16|154897|972-28-2152|\n",
      "|    Edwin|     Fritz|    Ehresman|     M|1959-10-02|236463|919-39-5692|\n",
      "|   Jimmie|  Lawrence|    Woodward|     M|1968-05-12| 37514|984-63-5828|\n",
      "|     Cami|  Josefine|        Neat|     F|1945-04-18|231234|985-46-8325|\n",
      "|     Gwyn|    Nichol|   Waltemath|     F|1934-07-09|297958|923-88-6201|\n",
      "|   Denver|      Shon|        Caho|     M|1971-12-17| 27360|921-25-7131|\n",
      "|     Jess|  Frederic|   Vankammen|     M|1942-10-02|107405|983-28-5981|\n",
      "| Mohammad|      Chad|      Cardle|     M|1985-04-21| 92744|927-17-9121|\n",
      "|    Perry|    August|      Gerbig|     M|1981-04-24|175376|988-20-4806|\n",
      "+---------+----------+------------+------+----------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.1).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the duplicate record**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+----------+------+-----------+-----------+----------+------------+---------+\n",
      "|firstName|middleName| lastName|gender| birthDate|salary|        ssn|lcFirstName|lcLastName|lcMiddleName|  ssnNums|\n",
      "+---------+----------+---------+------+----------+------+-----------+-----------+----------+------------+---------+\n",
      "|  Emanuel|   Wallace|   Panton|     M|1988-03-04|101255|935-90-7627|    emanuel|    panton|     wallace|935907627|\n",
      "|   Eloisa|     Rubye|Cayouette|     F|2000-06-20|204031|935-89-9009|     eloisa| cayouette|       rubye|935899009|\n",
      "|    Cathi|  Svetlana|    Prins|     F|2012-12-22| 35895|959-30-7957|      cathi|     prins|    svetlana|959307957|\n",
      "|  Mitchel|    Andres|Mozdzierz|     M|1966-05-06| 55108|989-27-8093|    mitchel| mozdzierz|      andres|989278093|\n",
      "|    Angla|     Melba|Hartzheim|     F|1938-07-26| 13199|935-27-4276|      angla| hartzheim|       melba|935274276|\n",
      "|   Rachel|    Marlin|Borremans|     F|1923-02-23| 67070|996-41-8616|     rachel| borremans|      marlin|996418616|\n",
      "| Catarina|  Phylicia|  Dominic|     F|1969-09-29|201021|999-84-8888|   catarina|   dominic|    phylicia|999848888|\n",
      "|  Antione|     Randy| Hamacher|     M|2004-03-05|271486|917-96-3554|    antione|  hamacher|       randy|917963554|\n",
      "| Madaline|  Shawanda| Piszczek|     F|1996-03-17|183944|963-87-9974|   madaline|  piszczek|    shawanda|963879974|\n",
      "|  Luciano|   Norbert|  Sarcone|     M|1962-12-14| 73069|909-96-1669|    luciano|   sarcone|     norbert|909961669|\n",
      "|   Newton|      Jose| Piacente|     M|1969-11-05|264422|967-61-8575|     newton|  piacente|        jose|967618575|\n",
      "|   Gretta|  Jennefer|  Dipinto|     F|1921-10-28|118497|907-49-2510|     gretta|   dipinto|    jennefer|907492510|\n",
      "|    Jenni|   Janella|Mcquiston|     F|1969-07-11|137418|932-55-7164|      jenni| mcquiston|     janella|932557164|\n",
      "|   Malena|     Apryl|    Kings|     F|1980-01-08| 98204|934-48-2334|     malena|     kings|       apryl|934482334|\n",
      "|    Willy|   Russell|   Merker|     M|1967-10-10| 41026|992-83-4829|      willy|    merker|     russell|992834829|\n",
      "|     Jami|     Allen| Mulkerin|     F|1958-08-08|236024|934-86-4126|       jami|  mulkerin|       allen|934864126|\n",
      "| Fernando|    Lowell|   Zebell|     M|1955-07-03|246516|951-57-6196|   fernando|    zebell|      lowell|951576196|\n",
      "|   Garnet|   Helaine|  Edghill|     F|1985-05-15| 97125|979-42-4409|     garnet|   edghill|     helaine|979424409|\n",
      "|      Siu|   Cherrie|  Lechelt|     F|2012-07-24|148331|906-85-3202|        siu|   lechelt|     cherrie|906853202|\n",
      "|   Randal|   Osvaldo|    Torma|     M|2013-12-17|275086|982-68-6906|     randal|     torma|     osvaldo|982686906|\n",
      "+---------+----------+---------+------+----------+------+-----------+-----------+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    "  .select(col(\"*\"),\n",
    "      lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "      lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "      lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n",
    "      translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "   ).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73cb8e56-0892-4ee4-b175-9cf1ca51ab30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "dedupedDF = (df\n",
    "  .select(col(\"*\"),\n",
    "      lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "      lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "      lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n",
    "      translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "   )\n",
    "  .dropDuplicates([\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\", \"gender\", \"birthDate\", \"salary\"])\n",
    "  .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+------+----------+------+-----------+\n",
      "|firstName|middleName|   lastName|gender| birthDate|salary|        ssn|\n",
      "+---------+----------+-----------+------+----------+------+-----------+\n",
      "|    Aaron|    Walker| Okoniewski|     M|1930-07-29| 97932|951-32-1950|\n",
      "|    Aaron|   Brendon|   Jernberg|     M|1924-09-26|277299|951-57-5457|\n",
      "|    Aaron| Alejandro|      Parbs|     M|1958-08-13| 10828|959-70-4852|\n",
      "|    Aaron|    Rashad|  Immediato|     M|1922-02-13| 38566|959-93-7472|\n",
      "|    Aaron|    Barton|     Crasco|     M|1986-11-21|298912|986-88-3115|\n",
      "|    Aaron|     Micah| Fotopoulos|     M|2010-02-02| 10842|995-82-1665|\n",
      "|    Abbie|    Evelin|     Nichol|     F|1985-01-10| 95861|919-95-6712|\n",
      "|    Abbie|     Marty|     Gungor|     F|1970-01-04| 45702|989-77-8677|\n",
      "|     Abby|       Mei|Hershnowitz|     F|1949-04-12|274011|666-90-8782|\n",
      "|     Abby|   Loraine|     Ligler|     F|1950-09-26|296309|926-91-5492|\n",
      "|     Abby|   Natisha|     Bermel|     F|2002-08-01|262392|928-90-4871|\n",
      "|     Abby|   Laurene|     Darbro|     F|2010-08-25|212129|992-78-9733|\n",
      "|    Abdul|  Jefferey|      Laxen|     M|1951-04-13| 52061|913-16-5194|\n",
      "|    Abdul| Valentine|      Nives|     M|1920-08-21| 82193|924-25-9193|\n",
      "|    Abdul|       Len|       Puna|     M|1966-10-04|183971|957-17-6021|\n",
      "+---------+----------+-----------+------+----------+------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dedupedDF.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the data as parquest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7e1812-51bb-4056-b076-ef445b89e440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we can save the results. We'll also re-read them and count them, just as a final check.\n",
    "(dedupedDF.write\n",
    "   .mode(\"overwrite\")\n",
    "   .parquet(destFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b39e357b-c86c-4ff4-b301-0fc1569462b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00001-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00002-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00003-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00004-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00005-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      ".part-00006-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet.crc\n",
      "._SUCCESS.crc\n",
      "part-00000-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00001-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00002-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00003-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00004-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00005-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "part-00006-1462c10c-1220-4f93-9880-c89d4d5b5464-c000.snappy.parquet\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in the directory\n",
    "file_list = os.listdir(destFile)\n",
    "\n",
    "# Display the list of files\n",
    "print(\"\\n\".join(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Solution - Exercise-Deduplication-of-Data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

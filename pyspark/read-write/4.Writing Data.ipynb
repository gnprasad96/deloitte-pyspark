{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4971ae6c-e2aa-4988-9ed9-7a895c879471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Writing Data\n",
    "\n",
    "Just as there are many ways to read data, we have just as many ways to write data.\n",
    "\n",
    "In this notebook, we will take a quick peek at how to write data back out to Parquet files.\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Writing data to Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b1de67d-a252-4cd0-bf9b-0d1899d967f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Read CSV Data\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4f69c1-8d26-42e6-96de-8c4ca042e522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Writing Data\n",
    "\n",
    "Let's start with one of our original CSV data sources, **pageviews_by_second.tsv**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a1addb-676c-4ab3-ad99-13754c33a7db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "  StructField(\"timestamp\", StringType(), False),\n",
    "  StructField(\"site\", StringType(), False),\n",
    "  StructField(\"requests\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "csvFile = \"../dataset/pageviews_by_second.tsv\"\n",
    "\n",
    "csvDF = (spark.read\n",
    "  .option('header', 'true')\n",
    "  .option('sep', \"\\t\")\n",
    "  .schema(csvSchema)\n",
    "  .csv(csvFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e8461f-292b-4d39-87b6-54afb5e71461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we have a `DataFrame`, we can write it back out as Parquet files or other various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc442c8-d25d-4f3e-9e47-e18c4a590f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory_path = \"../dataset/out/pageviews_by_second.parquet\"\n",
    "print(\"Output location: \" + directory_path)\n",
    "\n",
    "(csvDF.write                       # Our DataFrameWriter\n",
    "  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\n",
    "  .mode(\"overwrite\")               # Replace existing files\n",
    "  .parquet(directory_path)               # Write DataFrame to Parquet files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173b8e45-08fc-4a8b-a564-0f6860978b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that the file has been written out, we can see it in the DBFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ab2d94-02b3-4d77-9a5c-7e07d32a5179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List files in the directory\n",
    "file_list = os.listdir(directory_path)\n",
    "\n",
    "# Display the list of files\n",
    "print(\"\\n\".join(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e4a807b-f014-4ab3-bc62-55dadbd32f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And lastly we can read that same parquet file back in and display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc194c20-7d07-437d-afbe-7632e1037b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(directory_path).show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5.Writing Data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
